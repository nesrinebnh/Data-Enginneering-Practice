
// Instructions
// Write a Spark program in Scala to read a CSV file containing sales data (product, price, quantity, etc.). 
// Calculate the total revenue generated by each product and sort the results in descending order of revenue. 
// Print the top N products with the highest revenue.

import org.apache.spark.sql.{SparkSession, DataFrame}

// Create a SparkSession
val spark = SparkSession.builder().appName("CSV Reader").master("local[*]").getOrCreate()

// Read CSV file into a DataFrame
val csvPath = "/user/hadoop/Sales_April_2019.csv"
val csvOptions = Map("header" -> "true", "inferSchema" -> "true", "delimiter" -> ",")

val df: DataFrame = spark.read.format("csv").options(csvOptions).load(csvPath)

// Show the DataFrame
df.show()

// Create a new column for total price for an order 
val total_price: DataFrame = df.withColumn("Total Price", col("Quantity Ordered") * col("Price Each"))
total_price.show()

// Calculate the total revenue generated by each product
val total_revenue: DataFrame = total_price.groupBy("Product").agg(round(sum("Total Price"),2).alias("Total Revenue"))


// sort the results in descending order of revenue
val total_revenue_df: DataFrame = total_revenue.orderBy(col("Total Revenue").desc)

// Print the top N products with the highest revenue
total_revenue_df.select("Product").show(10)

